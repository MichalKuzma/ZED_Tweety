{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {
    "collapsed": true
   },
   "level": 1,
   "source": [
    "Import bibliotek i deklaracja wyrażeń regularnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "RE_SPACES = re.compile(\"\\s+\")\n",
    "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
    "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Klasa normalizująca text tweet'a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeforeTokenizationNormalizer:\n",
    "    @staticmethod\n",
    "    def normalize(text):\n",
    "        text = text.strip().lower()\n",
    "        text = text.replace('&nbsp;', ' ')\n",
    "        text = text.replace('&lt;', '<')\n",
    "        text = text.replace('&gt;', '>')\n",
    "        text = text.replace('&amp;', '&')\n",
    "        text = text.replace('&pound;', u'£')\n",
    "        text = text.replace('&euro;', u'€')\n",
    "        text = text.replace('&copy;', u'©')\n",
    "        text = text.replace('&reg;', u'®')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetTokenizer:\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            match = re.search(RE_EMOTICONS, token)\n",
    "            if match is None:\n",
    "                match = re.search(RE_HASHTAG, token)\n",
    "            if match is None:\n",
    "                match = re.search(RE_HTTP, token)\n",
    "\n",
    "            # sprawdź czy w ramach tokena występuje emotikona, hashtag lub link\n",
    "            if match is not None:\n",
    "                foundEmoticon = match.group(0)\n",
    "                emoticonStart = token.find(foundEmoticon)\n",
    "                emoticonEnd = emoticonStart + len(foundEmoticon)\n",
    "                newToken = token[0:emoticonStart] + token[emoticonEnd:]\n",
    "                tokens.append(match.group(0))\n",
    "                tokens[i] = newToken\n",
    "                i -= 1\n",
    "                # wydziel emotikonę lub hashtag jako token a resztę tekstu rozpatrz ponownie\n",
    "            else:\n",
    "                del tokens[i]\n",
    "                tokens[i:i] = nltk.word_tokenize(token)\n",
    "            i += 1\n",
    "\n",
    "        # stwórz stemmer i w pętli stemmuj wszystkie tokeny\n",
    "        stemmer = nltk.PorterStemmer()\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = stemmer.stem(tokens[i])\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Klasyfikator tweet'ów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassifier:\n",
    "    min_word_count = 5\n",
    "\n",
    "    stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
    "                 \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
    "                 \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\",\n",
    "                 \"him\",\n",
    "                 \"himself\", \"his\", \"how\", \"i\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\",\n",
    "                 \"my\",\n",
    "                 \"myself\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"sha\",\n",
    "                 \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "                 \"then\", \"there\", \"there's\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\",\n",
    "                 \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"with\", \"would\", \"you\",\n",
    "                 \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
    "                 \"n't\", \"'s\", \"'ll\", \"'re\", \"'d\", \"'m\", \"'ve\",\n",
    "                 \"above\", \"again\", \"against\", \"below\", \"but\", \"cannot\", \"down\", \"few\", \"if\", \"no\", \"nor\", \"not\", \"off\",\n",
    "                 \"out\", \"over\", \"same\", \"too\", \"under\", \"why\"]\n",
    "\n",
    "    def __init__(self, train_file, test_file):\n",
    "        self.feature_dict = {}\n",
    "        self.list_of_labels = []\n",
    "        self.words = Counter()\n",
    "        self.test_result = {}\n",
    "        self.classifier = RandomForestClassifier(n_estimators=300, n_jobs=1, random_state=23)\n",
    "        self.train_tweets = pd.read_csv(train_file, sep=\",\", na_values=[\"\"],\n",
    "                                        dtype={\"Id\": np.str, \"Category\": np.str, \"Tweet\": np.str}).dropna()\n",
    "        self.learn()\n",
    "        self.test_tweets = pd.read_csv(test_file, sep=\",\", na_values=[\"\"],\n",
    "                                       dtype={\"Id\": np.str, \"Tweet\": np.str}).dropna()\n",
    "\n",
    "    def learn(self):\n",
    "        self.compute_words()\n",
    "\n",
    "        X_train, y_train = self.create_bow(self.train_tweets, self.feature_dict)\n",
    "        self.list_of_labels = list(set(y_train))\n",
    "        self.classifier.fit(X_train, y_train)\n",
    "\n",
    "    def test(self):\n",
    "        self.compute_words()\n",
    "\n",
    "        X_test, y_test = self.create_bow(self.test_tweets, self.feature_dict)\n",
    "        predicted = self.classifier.predict(X_test)\n",
    "\n",
    "        self.test_result = {}\n",
    "        i = 0\n",
    "        for index, row in self.test_tweets.iterrows():\n",
    "            self.test_result[row[\"Id\"]] = predicted[i]\n",
    "            i += 1\n",
    "        return self.test_result\n",
    "\n",
    "    def save_test_result_to_file(self, file_name):\n",
    "        with open(file_name, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['Id', 'Category'])\n",
    "            for k in self.test_result.keys():\n",
    "                writer.writerow([k, self.test_result[k]])\n",
    "\n",
    "    def compute_words(self):\n",
    "        self.words.clear()\n",
    "        for index, row in self.train_tweets.iterrows():\n",
    "            tweet = BeforeTokenizationNormalizer.normalize(row[\"Tweet\"])\n",
    "            self.words.update(TweetTokenizer.tokenize(tweet))\n",
    "\n",
    "        for c in string.punctuation:\n",
    "            del self.words[c]\n",
    "\n",
    "        for word in self.stopwords:\n",
    "            del self.words[word]\n",
    "\n",
    "        common_words = list([k for k, v in self.words.most_common() if v > self.min_word_count])\n",
    "\n",
    "        self.feature_dict = {}\n",
    "        for word in common_words:\n",
    "            self.feature_dict[word] = len(self.feature_dict)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_bow(documents, features):\n",
    "        row = []\n",
    "        col = []\n",
    "        data = []\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        i = 0\n",
    "        for _, row_iter in documents.iterrows():\n",
    "            tweet = BeforeTokenizationNormalizer.normalize(row_iter[\"Tweet\"])\n",
    "            if \"Category\" in row_iter:\n",
    "                label = row_iter[\"Category\"]\n",
    "                labels.append(label)\n",
    "            tweet_tokens = TweetTokenizer.tokenize(tweet)\n",
    "\n",
    "            for token in set(tweet_tokens):\n",
    "                if token not in features:\n",
    "                    continue\n",
    "                row.append(i)\n",
    "                col.append(features[token])\n",
    "                data.append(1)\n",
    "            i += 1\n",
    "        return csr_matrix((data, (row, col)), shape=(len(documents), len(features))), labels"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Przykładowe wywołanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}